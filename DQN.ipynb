{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Deep Q-learning:__ approximates the Q function, the max expected value of the total reward over any and all successive steps, to then learn the optimal policy.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Key Pytorch functions:__\n",
    "    \n",
    "   - `torch.nn` neural net\n",
    "   - `torch.optim` - optimization \n",
    "   - `torch.autograd` - automatic differentiaion \n",
    "   - `torchvision` - utilities for vision tasks (separate package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "#import torchvisions.transforms as T\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory \n",
    "\n",
    "Stores the transitions $(s, a, r, s')$ that the agent observes to allow us to reuse the data later. By randomly sampling from this data, the transitions built up are decorrelated, which has been shown to greatly stabilize and improve the DQN training process. \n",
    "\n",
    "For this we need two classes: \n",
    "\n",
    "- `Transitions` - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result. \n",
    "\n",
    "- `ReplayMemory` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements '.sample()' method for selecting a random batch of transitions for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', \n",
    "                                       'action', \n",
    "                                       'next_state', \n",
    "                                       'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object): \n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"Components of replay memory\"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position+1) % self.capacity \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Randomly sample transitions to decorrelate data\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Algorithm \n",
    "\n",
    "__Environment:__ stochastic environment, thus the expectations are over stochastic transitions of our environment\n",
    "\n",
    "__Input:__ current state vector of the agent \n",
    "\n",
    "__Output:__ produces a Q-value for _every possible_ state-action pair of the current state in a single forward pass. \n",
    "\n",
    "__Tools__\n",
    "\n",
    "- __Experience replay & replay buffer:__ store each experience tuple in the replay buffer as agent interacts, then sample a small batch of tuples from it in order to learn. This process enables learning from the same experience multiple times, including rare events. \n",
    "- __Fixed Q-Targets:__ The main idea of introducing fixed Q-targets is that both _labels and predicted values_ are functions of the same weights. All the Q-values are intrinsically tied together through the function parameters. Break correlations between the target and the parameters that we are changing.\n",
    "\n",
    "__Goal:__ train a policy that tries to maximize the discounted, cumulative reward, where $R_{t_0}$ is the return: \n",
    "\n",
    "$R_{t0}=∑_{t=t_0}^∞γ^{t−t_0}r_t$.\n",
    "\n",
    "The idea behind Q-learning is that if we had a function: \n",
    "\n",
    "$Q^∗: State×Action→ℝ$ \n",
    "\n",
    "that could tell us what our return would be when we take a certain action in a given state, then we could construct a policy that maximizes our rewards:\n",
    "\n",
    "$π^∗(s)=argmax_a Q^∗(s,a)$\n",
    "\n",
    "But, because we don’t know everything about the world, we don’t have access to $Q^*$ so our network, as a universal function approximator, will be trained to approximate and resemble $Q^*$.\n",
    "\n",
    "The approximate action-value function: $\\hat{Q}(S,A,w) \\approx q_π(S,A)$\n",
    "\n",
    "__Linear Action-Value Function Approximation__\n",
    "\n",
    "Represent the state and action pairs using a feature vector: \n",
    "\n",
    "$x(S,A) = (x_1(S,A), ... , x_n(S,A))^T$\n",
    "\n",
    "Which lets us represent the action-value function by a linear combinatinon of features: \n",
    "\n",
    "$\\hat{q}(S,A,w) = x(S,A)^Tw = \\sum_{j=1}^{n}x_j(S,A)w_j$\n",
    "\n",
    "$\\hat{q}(S,A,w) = w_1f_1(s,a) + w_2f_2(s,a) +...+ w_nf_n(s,a)$ \n",
    "\n",
    "__Goal:__ To minimize the mean-squared error between the true action-value function, $q_π(S,A)$, and the approximate action-value function $\\hat{Q}(S,A,w) \\approx q_π(S,A)$, we use stochastic gradient descent with a differentiable function of the parameter vector w:\n",
    "\n",
    "$J(w) = \\mathbb{E}_π [(q_π(S,A) - \\hat{q}(S,A,w))^2]$\n",
    "\n",
    "__Update rule:__ every Q-value function for some policy obeys the Bellman equation:\n",
    "\n",
    "$Q^π(s,a)= r + γQ^π(s′,π(s′))$ \n",
    "\n",
    "Because we don't know the true action-value function, $q_π(S,A)$, then we have to substitute a target for this value. For Q-learning, we use the TD target: $R_{t+1} + γQ(S_{t+1},A_{t+1})$.\n",
    "\n",
    "__Linear VFA weight update__ \n",
    "\n",
    "The format for the weight update = step-size * prediction_error * feature_value, which is mathematically expressed as: \n",
    "\n",
    "$\\Delta w=α[(q_π(S, A)-\\hat{q}(S,A,w))\\nabla_w\\hat{q}(S,A,w)]$ \n",
    "\n",
    "Substituting the TD target yields the following:\n",
    "\n",
    "$\\Delta w = α(R_{t+1} + γ\\hat{q}(S_{t+1},A_{t+1},w) - \\hat{q}(S_t,A_t,w))\\nabla_w\\hat{q}(S_t,A_t,w) $ or \n",
    "\n",
    "$w_m \\leftarrow w_m + α[r + γ max_a Q(s',a') - Q(s,a)]f_m(s,a)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loss:__ <font color='red'>WIP</font>\n",
    "\n",
    "- $α = 0.05$, the learning rate\n",
    "- linear VFA update $\\Delta w$ \n",
    "- SGD with pytorch: ` torch.optim.sgd` - implments SGD with args params, learning_rate, momentum, weight_decay, dampening, and nesterov\n",
    "\n",
    "___Example of Pytorch Loss implementation:___\n",
    "\n",
    "```\n",
    "import torch\n",
    "from .optimizer import Optimizer, required\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.1, momentum=0.9)\n",
    "optimizer.zero_grad()\n",
    "loss_fn(model(input),target).backward()\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q-network__\n",
    "\n",
    "Our model will be a neural network that takes in the difference between the current and previous Q-values. If has outputs for every possible action? \n",
    "\n",
    "Reference for below code: https://towardsdatascience.com/building-neural-network-using-pytorch-84f6e75f9a\n",
    "\n",
    "__Description of the code below__\n",
    "\n",
    "`self.hidden = nn.Linear(784, 256)`\n",
    "\n",
    "The module automatically creates the weight and bias tensors which we'll use in the forward method. You can access the weight and bias tensors once the network (net) is created with net.hidden.weight and net.hidden.bias.\n",
    "\n",
    "`self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)`\n",
    "\n",
    "Defined operations for the sigmoid activation and softmax output. Setting dim=1 in nn.Softmax(dim=1) calculates softmax across the columns.\n",
    "\n",
    "`def forward(self, x):`\n",
    "\n",
    "PyTorch networks created with nn.Module must have a forward method defined. It takes in a tensor x and passes it through the operations you defined in the __ init __ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximate q-values of actions for current state  \n",
    "from torch import nn \n",
    "\n",
    "class Network(nn.Module): # class to track nn architecture\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Inputs to hidden layer: linear transformation \n",
    "        # 30 inputs; 20 outputs\n",
    "        self.hidden = nn.Linear(30,20) \n",
    "        \n",
    "        # Output layer: linear transformation - 20 inputs; 1 output  \n",
    "        # Does the size of the output depend on the # of actions?\n",
    "        self.output = nn.Linear(20,1)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
