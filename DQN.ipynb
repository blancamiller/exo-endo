{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Deep Q-learning:__ approximates the Q function, the max expected value of the total reward over any and all successive steps, to then learn the optimal policy.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Key Pytorch functions:__\n",
    "    \n",
    "   - `torch.nn` neural net\n",
    "   - `torch.optim` - optimization \n",
    "   - `torch.autograd` - automatic differentiaion \n",
    "   - `torchvision` - utilities for vision tasks (separate package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "#import torchvisions.transforms as T\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory \n",
    "\n",
    "Stores the transitions (s, a, r, s') that the agent observes to allow us to reuse the data later. By randomly sampling from this data, the transitions built up are decorrelated, which has been shown to greatly stabilize and improve the DQN training process. \n",
    "\n",
    "For this we need two classes: \n",
    "\n",
    "- `Transitions` - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result. \n",
    "\n",
    "- `ReplayMemory` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements '.sample()' method for selecting a random batch of transitions for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', \n",
    "                                       'action', \n",
    "                                       'next_state', \n",
    "                                       'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object): \n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position+1) % self.capacity \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Algorithm \n",
    "\n",
    "__Environment:__ stochastic, thus expectations over stochastic transitions in the environment\n",
    "\n",
    "__Goal:__ train a policy that tries to maximize the discounted, cumulative reward $R_{t0}=∑_{t=t_0}^∞γ^{t−t_0}r_t$, where $R_{t_0}$ is the return.\n",
    "\n",
    "The main idea behind Q-learning is that if we had a function $Q^∗:State×Action→ℝ$ that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "$π^∗(s)=argmax_a Q^∗(s,a)$\n",
    "\n",
    "But, we don’t know everything about the world, so we don’t have access to $Q^*$ so our network, as a universal function approximators, will be trained to approximate/resemble $Q^∗$.\n",
    "\n",
    "\n",
    "__Update rule:__ every Q-value function for some policy obeys the Bellman equation:\n",
    "\n",
    "$Q^π(s,a)=r+γQ^π(s′,π(s′))$\n",
    "\n",
    "__TD Error:__ δ, the difference between the two sides of the update rule's equality: \n",
    "\n",
    "$δ=Q(s,a)−(r+γmax_aQ(s′,a))$\n",
    "\n",
    "__Loss:__ <font color='red'>WIP</font>\n",
    "\n",
    "To minimize this error we use ... (e.g. Huber loss) - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q-network__\n",
    "\n",
    "Our model will be a neural network that takes in the difference between the current and previous Q values. If has outputs for every possible action? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call to determine next action \n",
    "def forward(self, x):\n",
    "    #bn = batchnorm & conv=convolutional net\n",
    "    x = F.relu(self.bn1(self.conv1(x))) \n",
    "    x = F.relu(self.bn2(self.conv2(x)))\n",
    "    x = F.relu(self.bn3(self.conv3(x)))\n",
    "    return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
