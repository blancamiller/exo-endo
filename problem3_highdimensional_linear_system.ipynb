{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: High dimensional linear system \n",
    "\n",
    "This experiment test how well the global and stepwise algorithms handle high-dimensional data. \n",
    "     \n",
    "### MDP\n",
    "\n",
    "- States: 30 dimensional state vector \n",
    "    - $X_t = [X_{1t}, ... ,X_{15t}]^T$ , 15 exogenous state variables \n",
    "    - $E_t = [E_{1t}, ... ,E_{15t}]^T$ , 15 endogenous state variables \n",
    "        \n",
    "- State transition function: \n",
    "    - $X_{t+1} = M_x * X_t + ϵ_x$\n",
    "    - $E_{t+1} = M_e * [E_t, X_t, A_t]^T + ϵ_e$\n",
    "    - where $M_x$ & $M_e$ are the transition functions for the exogenous MRP & endogenous MDP, generated according to $N(0,1)$\n",
    "    - $ϵ_x$ is the exogenous normal noise distribution $N(0, 0.09)$ and \n",
    "    - $ϵ_e$ is the endogenous normal noise distirbution $N(0, 0.04)$ \n",
    "\n",
    "- $s_0$: the initial state is a zero vector \n",
    "\n",
    "- The observed state vector is a linear mixture of the hidden exogenous & endogenous states defined as :\n",
    "    - $S_t = M * [E_t, X_t]^T$ where $M$ is $30x30$ element of the reals generated according to $N(0,1)$\n",
    "\n",
    "- Reward: $R_t = R_{xt} + R_{et}$ where $R_{xt}$ is the endogenous reward & $R_{et}$ is the exogenous reward \n",
    "\n",
    "\n",
    "### Experiments \n",
    "\n",
    "Neural Net: \n",
    "- input: 30 dimensional state vector of endogenous & exogenous components\n",
    "- layer(s): a single hidden layer of 20 tanh units \n",
    "- output: linear q function for each value of the current state, $s_t$  \n",
    "\n",
    "All 4 Q-learners: \n",
    "- observe the entire current state $s_t$\n",
    "- are initialized identically and employ the same random seed \n",
    "- ɣ = 0.9, the discount factor\n",
    "- α = 0.05, the learning rate\n",
    "\n",
    "Difference between Q-learners:\n",
    "- The \"full Q-learner\" is trained on the full reward  \n",
    "- The endogenous reward Q-learners are trained on the (estimated) endogenous reward \n",
    "- For the first $L$ steps, where $L=1000$ steps, the full reward is employed, & we collect a database of $(s,a,s,r')$ transitions \n",
    "- After these $L$ steps, we apply the Global & Stepwise algorithms to estimate $W_x$ & $W_e$\n",
    "- Then the algorithms fit a linear regression model $R_{exo}(W_x^Ts)$ to predict the reward $r$ as a function of the exogenous state $x=W_x^Ts$\n",
    "\n",
    "Q-function Use: the Q function is used for action selection \n",
    "- `softmax equation' - from NN output: \n",
    "$a_t \\sim π(s, a_t) = \\frac{\\exp(Q(s_t, a)/β)}{\\sum\\limits_{i}\\exp(Q(s_t, a)/β)}$\n",
    "\n",
    "- β = 1.0, the temperature for Boltzmann exploration\n",
    "- Used steepest descent solving in Manopt \n",
    "- The PCC constraint epsilon is 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning w/ Softmax Action Selection using Global & Stepwise Algorithms to Identify Exogenous State Variables & Rewards (procedural form):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 0: Initialize Values\n",
    "\n",
    "Initialize:\n",
    "   - Q values randomly\n",
    "   - temperature: beta = 1.0\n",
    "   - learning rate: alpha = 0.05\n",
    "   - discount: gamma = 0.9 (for all Q-learners)\n",
    "   - first state s0 as zero \n",
    "   - first L steps where full reward is used for all 4 Q-learners: L = 1000\n",
    "   - time-step for graph: T=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Collect transitions to predict the exogenous reward, R_exo\n",
    "\n",
    "__This is Q-learning with softmax action selection and the added detail of decomposing the reward into exogenous & endogenous components.__\n",
    "\n",
    "For each time-step from 0 to L: \n",
    "- Choose action from current state, a_t & s_t, using softmax policy, pi(s_t, a_t), derived from Q-values: \n",
    "```\n",
    "a_t = exp[Q(s_t, a_t)/temperature] / exp[Q(s_t,a_i)/temperature]\n",
    "```  \n",
    "- Store s_t & a_t in transition database\n",
    "- Take action, a_t, observe r, s_t+1, where r is computed as full reward \n",
    "\n",
    "```\n",
    "Compute the full reward: R_t = R_x,t + R_e,t\n",
    "    \n",
    "- R_exo,t = -3 * avg(X_t)    where X_t is the exogenous state transition\n",
    "- R_end,t = exp[-|avg(E_t)-1|]   where E_t is the endogenous state transition\n",
    "```\n",
    "- Store r_t & s_t+1 in transition database\n",
    "- Update Q-value: \n",
    "```\n",
    "Q(s_t, a_t) <-- Q(s_t, a_t) + alpha * [r + gamma * max_a (Q(s_t+1, a_t+1) - Q(s_t, a_t))]\n",
    "```\n",
    "    \n",
    "- Update current state as next state: \n",
    "```\n",
    "s <-- s'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Apply Algorithm 1 & 2 to the transition database to estimate W_x & W_e\n",
    "\n",
    "- Use database of full reward collected transitions (s,a,s',r) to compute estimate of projected next state of W_x & W_e \n",
    "\n",
    "\n",
    "- Compute exogenous reward: fit a linear regression model using estimate of W_x & W_e (we do this to predict the reward, r, as a function of the exogenous state, x = W_x^T * s in Part 3). Compute exogenous reward as: \n",
    "```\n",
    "R_exo = (W_x^T * s) \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "- Compute endogenous reward: as residual of linear regression model\n",
    "```\n",
    "R_end(s) = r - R_exo(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3: Endogenous Q-learner (global, stepwise & oracle) uses computed R_end for steps L to 5000  \n",
    "    \n",
    "For each time-step from L to 5000: \n",
    "\n",
    "- Choose action from current state, a_t & s_t, using softmax policy, pi(s_t, a_t), derived from Q-values: \n",
    "```\n",
    "a_t = exp[Q(s_t, a_t)/temperature] / exp[Q(s_t,a_i)/temperature]\n",
    "```    \n",
    "    \n",
    "- Take action, a_t, observe endogenous r_end, s_t+1, where r_end is as computed in PART 2??? \n",
    "    \n",
    "- Update Q-value: \n",
    "```\n",
    "Q(s_t, a_t) <-- Q(s_t, a_t) + alpha * [r + gamma * max_a (Q(s_t+1, a_t+1) - Q(s_t, a_t))]\n",
    "```\n",
    "    \n",
    "- Update current state as next state: \n",
    "```\n",
    "s <-- s'\n",
    "```\n",
    "   \n",
    "Loop until 5000 actions have been executed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__States:__ 30 dimensional, with exogenous & endogenous components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initial state is zero vector \n",
    "states = np.zeros((1,30))\n",
    "print(states.dtype)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__State Transitions:__ \n",
    "\n",
    "Implement the following:\n",
    "    - transition functions for the exo MRP & endo MDP generated according to N(0,1)\n",
    "        - M_x an element of the reals with dimensionality 15x15\n",
    "        - M_e an element of the reals with dimensionality 15x31\n",
    "        - where each row of each matrix is normalized to sum to 0.99 for stability  \n",
    "    - gaussian noise distribution variables\n",
    "        - Epsilon_x generated by N(0,0.09)\n",
    "        - Epsilon_e generated by N(0,0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef sum_sqrt(a):\\n    return np.sqrt(np.sum(np.abs(a)**2, axis=-1))\\n\\ndef apply_norm_along_axis(a):\\n    return np.apply_along_axis(np.linalg.norm, 1, a)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# functions to verify that l2-norm is working\n",
    "'''\n",
    "def sum_sqrt(a):\n",
    "    return np.sqrt(np.sum(np.abs(a)**2, axis=-1))\n",
    "\n",
    "def apply_norm_along_axis(a):\n",
    "    return np.apply_along_axis(np.linalg.norm, 1, a)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_x: \n",
      " [[-0.19980464  0.61460971 -0.45196971  0.75581167 -1.20171605  0.00672903\n",
      "   0.45975634  0.89457626  0.71817953 -0.34660912 -0.32183746  1.38967022\n",
      "   0.82310815  0.1207746  -0.40816803]\n",
      " [-1.10917495 -0.33628095  0.27467286  1.04742835 -0.21070254 -0.94300722\n",
      "   0.03290482 -0.39550715 -1.27413999  0.4075431   0.72290992  0.52010763\n",
      "  -1.34282918 -0.23713391 -1.26415575]\n",
      " [-0.04847917 -0.28284972 -0.23142608  0.00734481  0.13320666 -0.74726323\n",
      "  -1.02187831  0.19371526  0.5365637  -0.15997272  0.08980079  0.87552893\n",
      "   0.22054629 -0.28268772 -0.8599804 ]\n",
      " [ 0.96280222 -0.18158858 -2.04114334  0.14835314  0.12842795  0.46120091\n",
      "   0.40407166  0.30227455 -0.40884389 -0.14121077 -0.41893461  0.04790342\n",
      "  -0.34201052 -0.28968019 -0.48976432]\n",
      " [-0.77271337 -0.27485742  0.57828462 -1.05660614  2.18928176  0.2614925\n",
      "  -0.35108909  1.22945868 -0.21841512  1.16023069  0.48174255 -0.97862775\n",
      "   0.0152969   1.47962361  0.95035897]\n",
      " [-0.34564359  0.56996016  0.2879895  -1.48523143 -0.08137806 -0.55997494\n",
      "  -0.15897442  1.39546332 -0.48506525  0.74796392 -1.93336444 -1.74721497\n",
      "  -1.71219204  0.32147654 -1.20736727]\n",
      " [-0.30456391  1.22450731 -0.0821101  -1.3119452  -0.25269283 -0.49087734\n",
      "   1.01055187 -1.97723935  0.49300725 -0.95327943  0.62761691  0.32408105\n",
      "   0.11037435 -0.72813647  1.5895075 ]\n",
      " [ 1.03018565  0.61830459 -1.09680027 -0.05847811 -1.0176196   0.52659505\n",
      "  -1.08065286  0.26643777 -0.9470438   1.52518295 -2.03107638  0.57687519\n",
      "   1.21532577 -1.7223858  -0.64036414]\n",
      " [-0.9688473   0.81702533 -0.96321769  0.57617698  0.33466011  0.76266673\n",
      "  -1.3386354  -0.37105784  0.7502557   0.64999392  1.33235741 -0.70790913\n",
      "   2.25079927  1.64766815  0.26160537]\n",
      " [ 0.87763225 -2.17433635  1.35261422  0.37032798  2.03336351 -0.30226986\n",
      "  -0.60487261 -0.58820561 -1.17090759  1.49122959  0.49690804 -1.26389867\n",
      "   0.13670691  1.4574309  -0.76214222]\n",
      " [-0.73209324 -0.04128131 -0.98219739  0.09202561 -0.20723345 -0.5643118\n",
      "  -0.72922787  0.88358067  1.38936829 -0.85456175  0.25491387 -0.11590026\n",
      "   0.28379827  0.13190771  1.70496303]\n",
      " [ 1.16639604  0.57049081  0.60132733 -0.51633014 -0.33036935 -1.26510094\n",
      "   0.82764661  0.11441457 -0.0082087   0.27357845 -0.20712565  0.54681305\n",
      "  -0.48157502  0.97555089  0.21287447]\n",
      " [ 0.3590271  -0.71499761  0.76931909 -0.84277922  0.37267785 -0.17553241\n",
      "  -0.37009597 -0.54216108  0.73857919  0.70966172 -0.21927357  1.46542708\n",
      "  -0.42806957  0.95600457 -0.23106658]\n",
      " [-1.80160551  1.09074809  0.91699664 -1.03743959  0.67011545  0.41751628\n",
      "  -1.39995446  2.20678282 -0.1530943   0.65884578  1.87229225  0.75065985\n",
      "  -0.34900012 -0.77115282 -0.29688048]\n",
      " [ 0.17475602  0.18849566 -0.57934572 -0.24373792 -0.49376705  0.32855142\n",
      "  -0.57724265 -0.46586965 -1.16910685 -0.96445834  1.34128052  0.90159087\n",
      "  -0.51456471 -0.295312    0.54641044]]\n",
      "m_e: \n",
      " [[-8.83940991e-01  4.74532568e-01  8.50345101e-01 -3.71505892e-01\n",
      "   6.27647186e-01  6.14682572e-01  4.02121806e-01 -3.66231926e-02\n",
      "  -7.26702031e-01  2.39313702e+00  2.21112078e-01  1.30630270e+00\n",
      "   6.59930777e-01 -3.02739169e-01  9.06204202e-01]\n",
      " [ 1.07206268e+00  3.60173142e-01  2.07516068e-01  6.09640225e-02\n",
      "  -6.20664853e-02 -4.59916832e-02  5.46605496e-01 -8.41330823e-01\n",
      "  -1.10602015e+00 -5.12039309e-01 -1.47938353e+00 -1.34764163e+00\n",
      "  -6.84568969e-02  1.12659558e+00 -1.27898057e+00]\n",
      " [ 6.43227572e-01 -3.63711598e-02  1.52052887e+00  1.25586358e+00\n",
      "  -2.38650672e-01 -1.09355524e+00 -8.60611659e-01 -3.97689583e-02\n",
      "  -1.88081307e-01 -1.00182476e+00  1.09462499e-01 -2.22193415e+00\n",
      "   3.14406418e-02  3.31867192e-01 -1.97295278e-01]\n",
      " [ 8.03609300e-01 -6.81116769e-01 -1.29969094e+00  3.60981339e-01\n",
      "  -2.74260884e-01 -1.16025802e+00  2.81073880e-01 -1.71873557e-01\n",
      "  -3.04658955e-01 -1.37374784e-01  1.26040960e+00  1.47455732e+00\n",
      "   9.06581361e-01  9.76688388e-01 -9.21047056e-01]\n",
      " [-1.32400802e-01  1.07546131e+00 -5.77520435e-01  3.58928000e-01\n",
      "  -9.40622080e-01 -2.50385726e-03 -5.10287817e-01 -3.53870308e-01\n",
      "   1.10958979e+00  2.44764071e-01  1.33586346e+00  7.61247483e-01\n",
      "   9.09370788e-01  3.44937167e-01 -4.40775998e-01]\n",
      " [ 1.46036073e+00 -1.51779789e+00  1.37438217e+00 -2.78069183e-01\n",
      "   1.24733197e+00  1.08027781e+00  1.54263993e-01 -3.21219069e+00\n",
      "   9.29707125e-01 -1.57817316e+00  1.35009513e+00  6.86916249e-01\n",
      "   6.36052315e-02  9.27241626e-01 -7.22325413e-01]\n",
      " [-2.35856721e+00 -1.41698941e+00  3.62413964e-01  7.18338950e-02\n",
      "   1.14081153e+00  1.01723528e+00 -1.78055431e+00  7.23516024e-01\n",
      "   2.66647134e-01 -1.14767658e+00 -2.74484444e-01  1.37219345e+00\n",
      "  -1.53684369e-01  2.28621964e-01 -1.37756847e+00]\n",
      " [ 9.14748261e-01  7.24136645e-01  6.91668853e-02 -6.02472230e-01\n",
      "  -4.02064107e-01 -1.32833745e+00 -4.19564538e-01 -1.11822696e+00\n",
      "  -6.65910795e-01  7.57916606e-01  1.00189582e-01 -1.65212828e-01\n",
      "  -1.25893836e+00  4.00631404e-01  4.87991614e-01]\n",
      " [ 1.02475542e+00  1.89414279e+00 -3.62431680e-01  1.20578171e+00\n",
      "   1.67740870e+00  4.50265136e-01  8.24505953e-01 -8.21733149e-01\n",
      "   1.48346963e+00  7.89650506e-01  2.97573206e-01 -6.27756554e-01\n",
      "   3.34450832e-01  4.70328828e-01  8.83091701e-01]\n",
      " [-8.45721156e-02 -1.18200500e+00  4.01382800e-01  3.76501918e-01\n",
      "   5.22331894e-01 -2.29739935e-02  1.41392650e-01  2.94057737e-01\n",
      "  -1.22943709e+00  5.95122926e-01 -2.72629386e-01 -6.93490086e-01\n",
      "   8.19993410e-01 -8.90888118e-01  1.89270965e+00]\n",
      " [ 9.50745821e-01 -2.86579791e-01 -2.36861982e-01 -9.22284451e-01\n",
      "  -3.99652254e-01 -1.22255882e-01 -8.11904972e-01  4.07824013e-01\n",
      "  -7.33350053e-01  2.56402077e-02  1.04396691e+00 -1.05122657e-01\n",
      "  -1.24320264e+00  7.85333781e-02 -7.49192858e-02]\n",
      " [ 1.49512746e+00  1.21609550e+00  4.21504972e-01 -5.30118076e-01\n",
      "   6.30739732e-01  9.94586543e-01  6.28184001e-01 -1.02229855e-01\n",
      "  -1.19863004e+00 -5.16123614e-01 -2.45347766e-01 -1.69773920e-01\n",
      "   7.53252593e-01 -3.84685574e-01 -5.89128540e-01]\n",
      " [-2.75254528e-01  3.53687891e-01 -1.52231498e+00  2.54901826e-01\n",
      "   5.49152599e-01  9.65383281e-01 -2.16655664e+00 -1.25634517e+00\n",
      "  -7.43869343e-01 -1.31400592e+00  1.24028421e+00  8.48495721e-02\n",
      "  -6.99115716e-01 -1.91412383e-01 -8.02800692e-01]\n",
      " [-7.66674604e-01  1.11128373e+00  1.76945821e-01 -4.96082433e-01\n",
      "  -5.63410901e-01 -5.50356048e-01 -1.23722614e+00 -1.39480119e+00\n",
      "  -3.18612403e+00 -1.24434184e+00 -4.21930879e-01 -1.60110414e-01\n",
      "  -1.44925711e+00  1.03738633e+00  2.12039921e-01]\n",
      " [ 1.89075091e-01  6.68827302e-01 -2.17297918e+00 -1.55222314e+00\n",
      "   2.42463037e-01  9.12280982e-01  6.06614329e-01 -5.94958995e-01\n",
      "   8.41193047e-01  4.45424851e-01 -3.70894498e-01 -4.00094686e-02\n",
      "  -8.81031461e-01  2.58303013e-01 -5.78209263e-01]]\n"
     ]
    }
   ],
   "source": [
    "# transition functions for MRP & MDP; M_x is 15x15 & M_e is 15x31 for M 30x30\n",
    "m_x = np.random.randn(15,15)\n",
    "m_e = np.random.randn(15,15)\n",
    "print('m_x: \\n', m_x)\n",
    "print('m_e: \\n', m_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m_x L2-norm: \n",
      " [-0.19980464  0.61460971 -0.45196971  0.75581167 -1.20171605  0.00672903\n",
      "  0.45975634  0.89457626  0.71817953 -0.34660912 -0.32183746  1.38967022\n",
      "  0.82310815  0.1207746  -0.40816803]\n"
     ]
    }
   ],
   "source": [
    "# normalize transition functions M_x & M_e using L2 norm\n",
    "# np.linalg.norm(x, axis=1) is fastest way to compute the L2-norm\n",
    "# L2-norm: each row's squared elements sum to 1 \n",
    "m_x_norm = np.linalg.norm(m_x, axis=1)\n",
    "m_e_norm = np.linalg.norm(m_e, axis=1)\n",
    "print('m_x L2-norm: \\n', m_x[0])\n",
    "#print('m_x L2-norm: \\n', m_e)\n",
    "\n",
    "#print('sum: ', apply_norm_along_axis(m_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify l2-norm works\n",
    "#norm_m_x = sum_sqrt(m_x)\n",
    "#print(norm_m_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1367660024611725\n",
      "-0.307804032921971\n"
     ]
    }
   ],
   "source": [
    "# state noise distributions: exo is N(0,0.09) & endo is N(0,0.04)\n",
    "# N(mu, sigma^2) -> sigma * np.random.randn() + mu\n",
    "sigma_x  = 0.3\n",
    "sigma_e = 0.2\n",
    "epsilon_x = sigma_x * np.random.randn()\n",
    "epsilon_e = sigma_e * np.random.randn()\n",
    "print(epsilon_x)\n",
    "print(epsilon_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Transitions -- WIP \n",
    "# new_exo_state = Mx * previous_exo_state + epsilon_x\n",
    "# new_endo_state = Me * previous_endo_state + epsilon_e\n",
    "\n",
    "# update_x = mx * state_x + epsilon_x\n",
    "# update_e = me * state_e + epsilon_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy: action selection is done with \n",
    "#if exploration == \"boltzmann\":\n",
    "    #Choose an action probabilistically, with weights relative to the Q-values.\n",
    "    #Q_d, allQ = sess.run([q_net.Q_dist,q_net.Q_out],feed_dict={q_net.inputs:[s],q_net.Temp:e,q_net.keep_per:1.0})\n",
    "    #a = np.random.choice(Q_d[0],p=Q_d[0])\n",
    "    #a = np.argmax(Q_d[0] == a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax \n",
    "\n",
    "- actions are ranked and weighted according to value estimates \n",
    "- in this case we use a Boltzmann distribution \n",
    "- temperature value: \n",
    "        - high temperatures cause the actions to be nearly equiprobable\n",
    "        - low temperatures cause the actions to have a greater difference in selection probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_selection(Qs, time_step, temperature):\n",
    "    \n",
    "    # loop through number of N -- what is N/? \n",
    "    \n",
    "        # exp( Q(s_t, a) / temperature )\n",
    "        num = math.exp(Qs[time_step,i]/temp)\n",
    "        \n",
    "        # sum_i of exp( Q(s_t, a_i) / temperature )  \n",
    "        denom = sum(math.exp(val/temp) for val in Qs[time_step,:])\n",
    "        \n",
    "        # action_t ~ num/denom\n",
    "        action = num / denom\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boltzmann Implementation Resource: https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Add this to network to compute Boltzmann probabilities.\n",
    "Temp = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "Q_dist = slim.softmax(Q_out/Temp)\n",
    "\n",
    "#Use this for action selection.\n",
    "t = 0.5\n",
    "Q_probs = sess.run(Q_dist,feed_dict={inputs:[state],Temp:t})\n",
    "action_value = np.random.choice(Q_probs[0],p=Q_probs[0])\n",
    "action = np.argmax(Q_probs[0] == action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to a network to compute Boltzmann probabilities \n",
    "temperature = 1.0\n",
    "Q_dist = slim.softmax(Q_out/temperature)\n",
    "\n",
    "# Use this for action selection \n",
    "temperature = 1.0 \n",
    "Q_probs = sess.run(Q_dist, feed_dict={inputs: [state], temperature:t})\n",
    "action_value = np.random.choice(Q_probs[0], p=Q_probs[0])\n",
    "action = np.argmax(Q_probs[0] == action_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
