{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN for CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q-learning:__ learn an action-value function Q, to approximate q*. The Q function is used to approximate the reward, based on a state. It does so by calculating the expected future value from the agent's current state, s, and action, a.  \n",
    "\n",
    "The role of the policy is to choose (s,a) pairs to visit and update, but this isn't required. The only requirement is that all pairs continue to be updated, meaning actions can be selected using a different approach. \n",
    "\n",
    "References:\n",
    "\n",
    "1. DQN Tutorial with CarPole env.: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "2. Q-learning with Taxi env.: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "    - discrete state space: 5x5 grid=25 locations, 4 destinations, 5 passender location, therefore --> 5x5x4x5=500 possible states \n",
    "    - descrete action space: south, north, east, west, pickup, dropoff\n",
    "\n",
    "3. DQN: https://github.com/keon/deep-q-learning\n",
    "\n",
    "4. Deep Q-learning with NChain env.: https://adventuresinmachinelearning.com/reinforcement-learning-tutorial-python-keras/\n",
    "\n",
    "5. Deep Q-learning: https://adventuresinmachinelearning.com/category/reinforcement-learning/\n",
    "\n",
    "6. Discretizing using Uniform-Space Grid: https://github.com/mlsdpk/MountainCar-Gym/blob/master/notebook.ipynb\n",
    "\n",
    "7. DQN with FlappyBird: https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space size (4,)\n",
      "Action space size: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# initialize environment\n",
    "env = gym.make('CartPole-v1')\n",
    "print('State space size', env.observation_space.shape)\n",
    "print('Action space size:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using GPU with pytorch \n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib interactive mode\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory \n",
    "\n",
    "Experience replay memory will be used to store the agent's state transitions, allowing us to reuse the data later. -- FOR WHAT? \n",
    "\n",
    "By sampling from it randomly, the transitions that build up are decorrelated. It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
    "\n",
    "Q-learning gives our agent some memory? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss \n",
    "\n",
    "For game environments, the reward is usually defined as the game score. However, this isn't always the most accurate approach for maximizing the reward. \n",
    "\n",
    "We instead optimize the reward according to a loss. The loss is a value that indicates hwo far our prediction is from the actual target. Thus we subtract the target value "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
