{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "#import torchvisions.transforms as T\n",
    "\n",
    "from torch.autograd import Variable \n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Values \n",
    "# Q values randomly \n",
    "beta = 1.0 # Boltzmann temperature \n",
    "alpha = 0.05 # learning rate \n",
    "gamma = 0.9 # discount rate\n",
    "L = 1000 # first L steps is when full reward is used for 4 Q-learners\n",
    "T = 1 # time-step\n",
    "#s_initial = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NN Model Definition & Architecture:__ \n",
    "    - Input Layer: 30 units since there are 15 exogenous & 15 endogenous components to each state  \n",
    "    - Output Layer: 30 units  \n",
    "    - Hiddne Layer: number of actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module): \n",
    "    \"\"\"\n",
    "    Define a the architecture of the neural net to learn the weights/\n",
    "    approximate the Q-values for each action in given current state. \n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        # input & output neurons \n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        \n",
    "        # linear layers Ax + b = ([weight x input] + bias)\n",
    "        # layers: input, output and hidden \n",
    "        self.fc1_input = nn.Linear(input_size, 20)\n",
    "        self.fc2_hidden = nn.Linear(20, 1)\n",
    "        self.fc3_output = nn.Linear(20, nb_action)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.tanh(self.fc1(state)) \n",
    "        x = F.tanh(self.fc2(x))\n",
    "        q_values = self.fc3(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c2d04543af3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-4cb2b96bb8ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# input & output neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# linear layers Ax + b = ([weight x input] + bias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_action' is not defined"
     ]
    }
   ],
   "source": [
    "net = NN(10)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tuple for transitions (s, a, r, s')\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object): \n",
    "    \"\"\"\n",
    "    Collect 1000 transitions (s, a, r, s') that will be randomly \n",
    "    sampled to stabilize and improve the DQN training process. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Components of replay memory\"\"\"\n",
    "        self.capacity = capacity # limit of storage\n",
    "        self.memory = [] # storage \n",
    "        self.position = 0 # storage starting position\n",
    "        \n",
    "    def push_transxn(self, *args):\n",
    "        \"\"\"Saves one transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None) \n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position+1) % self.capacity\n",
    "    \n",
    "    def sample_transxn(self, batch_size):\n",
    "        \"\"\"Randomly sample transitions to decorrelate data\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-16-4ec45bff0efc>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-4ec45bff0efc>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def select_action(self, state):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, input_size, nb_action, gamma, lrate, T):\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action according to Boltzmann softmax\"\"\"\n",
    "        \n",
    "    def learn(self, batch_size, batch_next_state, \n",
    "              batch_reward, batch_action):\n",
    "        \"\"\"\"\"\"\n",
    "        \n",
    "    def update_model(self, reward, new_signal):\n",
    "        \n",
    "        # after loss backward() (when the gradients are computed), \n",
    "        # we use the optimizer step to zero out the gradients as \n",
    "        # this does not happed automatically\n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "        samples, idxs, IS = self.sample()\n",
    "        Qpredict, Qtarget = self.calcTD(samples)\n",
    "        \n",
    "        for i in range(self.mbsize):\n",
    "            error = math.fabs(float(Qpredict[i] - Qtarget[i]))\n",
    "            self.replay.update(idxs[i], error)\n",
    "            \n",
    "        Jtd = self.loss()\n",
    "        JE = self.JE(samples)\n",
    "        Jn = self.Jn(samples, Qpredict)\n",
    "        J = Jtd + self.lambda2 * JE + self.lambda1 * Jn\n",
    "        J.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        if self.c >= self.C:\n",
    "            self.c = 0\n",
    "            self.vs.updateTargetNet()\n",
    "        else: \n",
    "            self.c += 1\n",
    "            \n",
    "        \n",
    "    #def score(self):\n",
    "             \n",
    "    #def save(self):\n",
    "           \n",
    "    #def load(self):   \n",
    "        \n",
    "    #def plot_durations(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent():\n",
    "    \n",
    "    def nn_model(self):\n",
    "        \n",
    "        self.ao = a0 # input_layer: shape=[1, in_units]\n",
    "        self.y = y # output_layer: shape=[1, out_units]\n",
    "        \n",
    "        # input layer -> hidden layer \n",
    "        self.w1 = weight # A - hidden units\n",
    "        self.b1 = bias # b - hidden units\n",
    "        self.a1 = tanh # output of the hidden layer\n",
    "    \n",
    "        # hidden layer -> ouput layer\n",
    "        self.w2 = # weight: hidden units & out units \n",
    "        self.b2 = # bias: out units \n",
    "        \n",
    "        # q-value and action \n",
    "        self.a2 = # (a1*w2)+b2-predicted_y Q-value of 4 actions\n",
    "        self.action = bias # agent would take the action which has max Q-value\n",
    "        \n",
    "        # loss function: mean square error \n",
    "        self.loss = sum(square(best_q_value-q_values))\n",
    "        \n",
    "        # update model \n",
    "        self.update_model = GradDesOptimizer(lrate=0.05).minimize(self.loss)\n",
    "        \n",
    "    def train(): \n",
    "        \n",
    "        # get hyperparameters \n",
    "        max_episodes = self.max_episodes\n",
    "        max_actions = self.max_actions\n",
    "        discount = self.discount\n",
    "        exploration_rate = self.exploration_rate\n",
    "        exploration_decay = self.exploration_decay \n",
    "        \n",
    "        # start training \n",
    "        for i in range(max_episodes):\n",
    "            state = env.reset()\n",
    "            for j in range(max_actions)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practicing from DQN_Refernce1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
