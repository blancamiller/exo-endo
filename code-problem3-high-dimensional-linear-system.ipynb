{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Deep Q-learning Steps:__\n",
    "1. Store all experience, transitions, in memory --> Replay memory\n",
    "2. Determine the next action according to the maximum output of the Q-network \n",
    "3. Compute the loss funciton using the mean squared error of the predicted Q-value and the target Q-value (essentially regression: y_hat-y). \n",
    "    - Issue: we don't know the target or predicted Q-value\n",
    "    - Solution: use the bellman Q-value equation:\n",
    "    $Q(S_t,A_t) = Q(S_t,A_t) + \\alpha[R_{t+1} + \\gamma max_aQ(S_t+1,a) - Q(S_t,A_t)]$ \n",
    "        - Q(S_t,A_t) is current q-value, we know this \n",
    "        - estimate TD-target: $R_t+1 + \\gamma max_aQ(S_{t+1},a)$ using a neural network; we don't know this         \n",
    "        \n",
    "        \n",
    "Reference: https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDependencies \\npython: 1.0?\\ntorch: 3.5?\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dependencies \n",
    "python: 1.0?\n",
    "torch: 3.5?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #evaluates matrix mult. & dot product btwn vectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "#import torchvisions.transforms as T\n",
    "\n",
    "from torch.autograd import Variable \n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pseudocode for deep Q-learning:__\n",
    "\n",
    "Initialize $Q_0(s,a)$ for all $s,a$ \n",
    "<br/>\n",
    "Get initial state, $s_0$\n",
    "<br/>\n",
    "For $k = 1...$ till convergence\n",
    "- sample action $a_t$ and get next state s'\n",
    "- If s' is terminal:\n",
    "    - target = $R(s,a,s')$\n",
    "    - sample new initial state s'\n",
    "- Else:\n",
    "    - TD_target = $R(s,a,s') + \\gamma max_aQ_k(s',a)$  \n",
    "    - $\\theta_{k+1} = \\theta_k - \\alpha $\n",
    "    - $s = s'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Define Initial Variables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "BETA = 1.0     # Boltzmann temperature \n",
    "LR = 0.05      # learning rate \n",
    "GAMMA = 0.9    # discount rate\n",
    "L = 1000       # first L steps, full reward is used for 4 Q-learners\n",
    "T = 1          # time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Q_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize s_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Define Initial Classes for DQN Loop  \n",
    "- Replay Memory Class \n",
    "- NN Class \n",
    "- DQN Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experience replay / replay memory class\n",
    "\n",
    "class ReplayMemory(object): \n",
    "    \"\"\"\n",
    "    Collect 1000 transitions, (s, a, r, s'), that will be randomly \n",
    "    sampled to stabilize and improve the DQN training process. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Components of replay memory\"\"\"\n",
    "        self.capacity = capacity # storage limit\n",
    "        self.memory = []         # storage \n",
    "        self.position = 0        # storage starting position\n",
    "        \n",
    "    def push_transxn(self, *args):\n",
    "        \"\"\"Saves one transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None) \n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position+1) % self.capacity\n",
    "    \n",
    "    def sample_transxn(self, batch_size):\n",
    "        \"\"\"Randomly sample transitions to decorrelate data\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.memory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NN Model Definition & Architecture:__ \n",
    "- Input Layer: 30 units for the 15 exogenous & 15 endogenous state components\n",
    "- Hidden Layer: 20 tanh units  \n",
    "- Output Layer: number of actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module): \n",
    "    \"\"\"\n",
    "    Define a the architecture of the neural net to learn the weights/\n",
    "    approximate the Q-values for each action in given current state. \n",
    "    \"\"\"    \n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        # input & output neurons \n",
    "        self.input_size = input_size\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # linear layers Ax + b = ([weight x input] + bias)\n",
    "        # fully connected layers: input, hidden and output \n",
    "        # (num_of_inputs_to_units, num_of_units)\n",
    "        self.fc1 = nn.Linear(30, 20) # fc1 is object variable\n",
    "        self.hidden = nn.Linear(20, 20) \n",
    "        self.output = nn.Linear(20, num_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state) \n",
    "        x = F.tanh(self.hidden(x))\n",
    "        q_values = self.output(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c2d04543af3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-535935de224b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# input & output neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# linear layers Ax + b = ([weight x input] + bias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_actions' is not defined"
     ]
    }
   ],
   "source": [
    "net = NN(10)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DQN Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tuple for transitions (s, a, r, s')\n",
    "Transition = namedtuple('Transition', \n",
    "                        ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \n",
    "    def __init__(self, input_size, nb_action, gamma, lrate, T):\n",
    "        self.input_size\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action according to Boltzmann softmax\"\"\"\n",
    "        return a\n",
    "        \n",
    "    def learn(self, batch_size, batch_next_state, \n",
    "              batch_reward, batch_action):\n",
    "        \"\"\"WIP\"\"\"\n",
    "        return q_value\n",
    "        \n",
    "    def update_model(self, reward, new_signal):\n",
    "        \n",
    "        # after loss backward() (when the gradients are computed), \n",
    "        # we use the optimizer step to zero out the gradients as \n",
    "        # this does not happed automatically\n",
    "        self.opt.zero_grad()\n",
    "        \n",
    "        samples, idxs, IS = self.sample()\n",
    "        Qpredict, Qtarget = self.calcTD(samples)\n",
    "        \n",
    "        for i in range(self.mbsize):\n",
    "            error = math.fabs(float(Qpredict[i] - Qtarget[i]))\n",
    "            self.replay.update(idxs[i], error)\n",
    "            \n",
    "        Jtd = self.loss()\n",
    "        JE = self.JE(samples)\n",
    "        Jn = self.Jn(samples, Qpredict)\n",
    "        J = Jtd + self.lambda2 * JE + self.lambda1 * Jn\n",
    "        J.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        if self.c >= self.C:\n",
    "            self.c = 0\n",
    "            self.vs.updateTargetNet()\n",
    "        else: \n",
    "            self.c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for number of steps\n",
    "\n",
    "    # sample action a_t according to boltzmann softmax \n",
    "    \n",
    "    # get next state s' \n",
    "    \n",
    "    # if s' is terminal \n",
    "        # set target to R(s,a,s')\n",
    "        # sample new initial state s'\n",
    "        \n",
    "    # else\n",
    "        # compute TD target using NN\n",
    "        \n",
    "        # compute new parameter theta_k+1 = theta_k - alpha\n",
    "        \n",
    "        # set current state as next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define Exo-Endo Global & Stepwise Algorithms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 4: Implement Exo-Endo Problem 3 using Parts 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-16-4ec45bff0efc>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-4ec45bff0efc>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    def select_action(self, state):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQAgent():\n",
    "    \n",
    "    def nn_model(self):\n",
    "        \n",
    "        self.ao = a0 # input_layer: shape=[1, in_units]\n",
    "        self.y = y # output_layer: shape=[1, out_units]\n",
    "        \n",
    "        # input layer -> hidden layer \n",
    "        self.w1 = weight # A - hidden units\n",
    "        self.b1 = bias # b - hidden units\n",
    "        self.a1 = tanh # output of the hidden layer\n",
    "    \n",
    "        # hidden layer -> ouput layer\n",
    "        self.w2 = # weight: hidden units & out units \n",
    "        self.b2 = # bias: out units \n",
    "        \n",
    "        # q-value and action \n",
    "        self.a2 = # (a1*w2)+b2-predicted_y Q-value of 4 actions\n",
    "        self.action = bias # agent would take the action which has max Q-value\n",
    "        \n",
    "        # loss function: mean square error \n",
    "        self.loss = sum(square(best_q_value-q_values))\n",
    "        \n",
    "        # update model \n",
    "        self.update_model = GradDesOptimizer(lrate=0.05).minimize(self.loss)\n",
    "        \n",
    "    def train(): \n",
    "        \n",
    "        # get hyperparameters \n",
    "        max_episodes = self.max_episodes\n",
    "        max_actions = self.max_actions\n",
    "        discount = self.discount\n",
    "        exploration_rate = self.exploration_rate\n",
    "        exploration_decay = self.exploration_decay \n",
    "        \n",
    "        # start training \n",
    "        for i in range(max_episodes):\n",
    "            state = env.reset()\n",
    "            for j in range(max_actions)\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practicing from DQN_Refernce1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
