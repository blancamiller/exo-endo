{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import random # random samples from different batches (experience replay)\n",
    "import os # For loading and saving brain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim # for using stochastic gradient descent\n",
    "import torch.autograd as autograd # Conversion from tensor (advanced \n",
    "# arrays) to avoid all that contains a gradient\n",
    "\n",
    "# We want to put the tensor into a varaible taht will also contain a\n",
    "# gradient and to this we need:\n",
    "from torch.autograd import Variable\n",
    "# to convert this tensor into a variable containing the tensor and the gradient\n",
    "\n",
    "# Creating the architecture of the Neural Network\n",
    "class Network(nn.Module): #inherinting from nn.Module\n",
    "\n",
    "    #Self - refers to the object that will be created from this class\n",
    "    #     - self here to specify that we're referring to the object\n",
    "    def __init__(self, input_size, nb_action): #[self,input neuroner, output neuroner]\n",
    "        super(Network, self).__init__() #inorder to use modules in torch.nn\n",
    "        # Input and output neurons\n",
    "        self.input_size = input_size\n",
    "        self.nb_action = nb_action\n",
    "        # Full connection between different layers of NN\n",
    "        # In this example its one input layer, one hidden layer & one output layer\n",
    "        # Using self here to specify that fc1 is a variable of my object\n",
    "        self.fc1 = nn.Linear(input_size, 40)\n",
    "        self.fc2 = nn.Linear(40, 30)\n",
    "        #Example of adding a hiddenlayer\n",
    "        # self.fcX = nn.Linear(30,30)\n",
    "        self.fc3 = nn.Linear(30, nb_action) # 30 neurons in hidden layer\n",
    "\n",
    "    # For function that will activate neurons and perform forward propagation\n",
    "    def forward(self, state):\n",
    "        # rectifier function\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_values = self.fc3(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Experience Replay\n",
    "# We know that RL is based on MDP\n",
    "# So going from one state(s_t) to the next state(s_t+1)\n",
    "# We gonna put 100 transition between state into what we call the memory\n",
    "# So we can use the distribution of experience to make a decision\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity #100 transitions\n",
    "        self.memory = [] #memory to save transitions\n",
    "\n",
    "    # pushing transitions into memory with append\n",
    "    #event=transition\n",
    "    def push(self, event):\n",
    "        self.memory.append(event)\n",
    "        if len(self.memory) > self.capacity: #memory only contain 100 events\n",
    "            del self.memory[0] #delete first transition from memory if there is more that 100\n",
    "\n",
    "    # taking random sample\n",
    "    def sample(self, batch_size):\n",
    "        #Creating variable that will contain the samples of memory\n",
    "        #zip =reshape function if list = ((1,2,3),(4,5,6)) zip(*list)= (1,4),(2,5),(3,6)\n",
    "        #                (state,action,reward),(state,action,reward)  \n",
    "        samples = zip(*random.sample(self.memory, batch_size))\n",
    "        #This is to be able to differentiate with respect to a tensor\n",
    "        #and this will then contain the tensor and gradient\n",
    "        #so for state,action and reward we will store the seperately into some\n",
    "        #bytes which each one will get a gradient\n",
    "        #so that eventually we'll be able to differentiate each one of them\n",
    "        return map(lambda x: Variable(torch.cat(x, 0)), samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Deep Q Learning\n",
    "\n",
    "class Dqn():\n",
    "\n",
    "    def __init__(self, input_size, nb_action, gamma, lrate, T):\n",
    "        self.gamma = gamma #self.gamma gets assigned to input argument\n",
    "        self.T = T\n",
    "        # Sliding window of the evolving mean of the last 100 events/transitions\n",
    "        self.reward_window = []\n",
    "        #Creating network with network class\n",
    "        self.model = Network(input_size, nb_action)\n",
    "        #creating memory with memory class\n",
    "        #We gonna take 100000 samples into memory and then we will sample from this memory to \n",
    "        #to get a snakk number of random transitions\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        #creating optimizer (stochastic gradient descent)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr = lrate) #learning rate\n",
    "        #input vector which is batch of input observations\n",
    "        #by unsqeeze we create a fake dimension to this is\n",
    "        #what the network expect for its inputs\n",
    "        #have to be the first dimension of the last_state\n",
    "        self.last_state = torch.Tensor(input_size).unsqueeze(0)\n",
    "        #Inilizing\n",
    "        self.last_action = 0\n",
    "        self.last_reward = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        #Q value depends on state\n",
    "        #Temperature parameter T will be a positive number and the closer\n",
    "        #it is to ze the less sure the NN will when taking an action\n",
    "        #forexample\n",
    "        #softmax((1,2,3))={0.04,0.11,0.85} ==> softmax((1,2,3)*3)={0,0.02,0.98} \n",
    "        #to deactivate brain then set T=0, thereby it is full random\n",
    "        probs = F.softmax((self.model(Variable(state, volatile = True))*self.T),dim=1) # T=100\n",
    "        #create a random draw from the probability distribution created from softmax\n",
    "        action = probs.multinomial()\n",
    "        print(probs.multinomial())\n",
    "        return action.data[0,0]\n",
    "\n",
    "    # See section 5.3 in AI handbook\n",
    "    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n",
    "        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "        #next input for target see page 7 in attached AI handbook\n",
    "        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n",
    "        target = self.gamma*next_outputs + batch_reward\n",
    "        #Using hubble loss inorder to obtain loss\n",
    "        td_loss = F.smooth_l1_loss(outputs, target)\n",
    "        #using  lass loss/error to perform stochastic gradient descent and update weights \n",
    "        self.optimizer.zero_grad() #reintialize the optimizer at each iteration of the loop\n",
    "        #This line of code that backward propagates the error into the NN\n",
    "        #td_loss.backward(retain_variables = True) #userwarning\n",
    "        td_loss.backward(retain_graph = True)\n",
    "        #And this line of code uses the optimizer to update the weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update(self, reward, new_signal):\n",
    "        #Updated one transition and we have dated the last element of the transition\n",
    "        #which is the new state\n",
    "        new_state = torch.Tensor(new_signal).float().unsqueeze(0)\n",
    "        self.memory.push((self.last_state, new_state, torch.LongTensor([int(self.last_action)]), torch.Tensor([self.last_reward])))\n",
    "        #After ending in a state its time to play a action\n",
    "        action = self.select_action(new_state)\n",
    "        if len(self.memory.memory) > 100:\n",
    "            batch_state, batch_next_state, batch_action, batch_reward = self.memory.sample(100)\n",
    "            self.learn(batch_state, batch_next_state, batch_reward, batch_action)\n",
    "        self.last_action = action\n",
    "        self.last_state = new_state\n",
    "        self.last_reward = reward\n",
    "        self.reward_window.append(reward)\n",
    "        if len(self.reward_window) > 1000:\n",
    "            del self.reward_window[0]\n",
    "        return action\n",
    "\n",
    "    def score(self):\n",
    "        return sum(self.reward_window)/(len(self.reward_window)+1.)\n",
    "\n",
    "    def save(self):\n",
    "        torch.save({'state_dict': self.model.state_dict(),\n",
    "                    'optimizer' : self.optimizer.state_dict(),\n",
    "                   }, 'last_brain.pth')\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.isfile('last_brain.pth'):\n",
    "            print(\"=> loading checkpoint... \")\n",
    "            checkpoint = torch.load('last_brain.pth')\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"done !\")\n",
    "        else:\n",
    "            print(\"no checkpoint found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://stackoverflow.com/questions/49065222/implementing-rnn-and-lstm-into-dqn-pytorch-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
